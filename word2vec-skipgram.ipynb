{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms = punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in words if w.lower() not in remove_terms]\n",
    "    stopw = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopw]\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = [lemma.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('History_of_Astronomy.txt', encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocessing(sentence) for sentence in corpus if sentence.strip() !='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 300)       1263600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 300)       1263600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,527,200\n",
      "Trainable params: 2,527,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim_embeddings = 300\n",
    "\n",
    "#inputs\n",
    "inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(input_dim=vocab_size, output_dim=dim_embeddings)(inputs)\n",
    "\n",
    "#context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c = Embedding(input_dim=vocab_size, output_dim=dim_embeddings)(c_inputs)\n",
    "\n",
    "d = Dot(axes=2)([w, c])\n",
    "d = Reshape((1,), input_shape=(1, 1))(d)\n",
    "d = Activation('sigmoid')(d)\n",
    "\n",
    "model = Model(inputs=[inputs, c_inputs], outputs=d)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 2338.1833555549383\n",
      "Epoch: 1 \tLoss: 1900.0107424929738\n",
      "Epoch: 2 \tLoss: 1609.4854783304036\n",
      "Epoch: 3 \tLoss: 1321.2070938460529\n",
      "Epoch: 4 \tLoss: 1064.508568521589\n",
      "Epoch: 5 \tLoss: 858.0785472672433\n",
      "Epoch: 6 \tLoss: 700.4538713130169\n",
      "Epoch: 7 \tLoss: 574.8111935441848\n",
      "Epoch: 8 \tLoss: 502.353528737789\n",
      "Epoch: 9 \tLoss: 438.83482115293737\n",
      "Epoch: 10 \tLoss: 402.5674714980123\n",
      "Epoch: 11 \tLoss: 368.22985522680756\n",
      "Epoch: 12 \tLoss: 348.0301035082739\n",
      "Epoch: 13 \tLoss: 322.16826397892146\n",
      "Epoch: 14 \tLoss: 310.69731902621425\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "for epoch in range(n_epochs):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(X_train_tokens):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=vocab_size, window_size=4)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += model.train_on_batch(x, y)\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word2vec-skipgrams1.txt' ,'w', encoding=\"utf8\")\n",
    "f.write('{} {}\\n'.format(vocab_size-1, dim_embeddings))\n",
    "\n",
    "weights = model.get_weights()[0]\n",
    "for word, i in items:\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(weights[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('word2vec-skipgrams1.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('substitute', 0.5043259263038635),\n",
       " ('advocated', 0.4944961667060852),\n",
       " ('physically', 0.486220121383667),\n",
       " ('recommend', 0.47913214564323425),\n",
       " ('pythagorean', 0.47744858264923096),\n",
       " ('relationship', 0.46811461448669434),\n",
       " ('unfit', 0.46585965156555176),\n",
       " ('simplification', 0.4655781686306),\n",
       " ('prospect', 0.46493586897850037),\n",
       " ('supposes', 0.4534285068511963)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
